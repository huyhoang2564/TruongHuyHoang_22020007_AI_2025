import torch
import math
import tiktoken

from previous_chapters import GPTModel, create_dataloader_v1
from my_train_domain import GPT_CONFIG_TINY, load_domain_text


def build_model(device):
    """Load Tiny GPT model ƒë√£ train t·ª´ file .pth"""
    model = GPTModel(GPT_CONFIG_TINY)
    state = torch.load("tiny_gpt_domain.pth", map_location=device)
    model.load_state_dict(state)
    model.to(device)
    model.eval()
    return model


def compute_perplexity(model, text, device, batch_size=8):
    """T√≠nh loss & perplexity tr√™n m·ªôt ƒëo·∫°n text (v√≠ d·ª•: validation set)."""
    tokenizer = tiktoken.get_encoding("gpt2")
    context_len = GPT_CONFIG_TINY["context_length"]

    loader = create_dataloader_v1(
        text,
        batch_size=batch_size,
        max_length=context_len,
        stride=context_len,
        drop_last=False,
        shuffle=False,
        num_workers=0,
    )

    losses = []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            logits = model(x)
            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                y.view(-1),
            )
            losses.append(loss.item())

    mean_loss = sum(losses) / len(losses)
    perplexity = math.exp(mean_loss)
    return mean_loss, perplexity


# ====== SAMPLING FUNCTIONS (kh√¥ng d√πng TensorFlow, kh√¥ng import file kh√°c) ======

def sample_next_token(logits, temperature=1.0, top_k=None):
    """L·∫•y 1 token ti·∫øp theo t·ª´ logits v·ªõi temperature + top-k sampling."""
    # √Åp d·ª•ng temperature
    logits = logits / temperature

    # √Åp d·ª•ng top-k filtering
    if top_k is not None:
        values, _ = torch.topk(logits, top_k)
        min_topk = values.min()
        logits[logits < min_topk] = -float("inf")

    # Softmax ‚Üí x√°c su·∫•t
    probs = torch.softmax(logits, dim=-1)

    # L·∫•y m·∫´u 1 token theo ph√¢n ph·ªëi x√°c su·∫•t
    next_id = torch.multinomial(probs, num_samples=1)
    return next_id.item()


def generate(model, idx, max_new_tokens, context_length,
             temperature=1.0, top_k=None):
    """
    Generate text token-by-token v·ªõi sampling.
    idx: tensor shape (1, T) ch·ª©a prompt ƒë√£ encode.
    """
    for _ in range(max_new_tokens):
        # Gi·ªõi h·∫°n context cho ph√π h·ª£p context_length
        idx_cond = idx[:, -context_length:]

        with torch.no_grad():
            logits = model(idx_cond)          # (B, T, vocab)
            logits = logits[:, -1, :]         # ch·ªâ l·∫•y token cu·ªëi (B, vocab)

        next_token_id = sample_next_token(
            logits[0],
            temperature=temperature,
            top_k=top_k,
        )

        next_token = torch.tensor(
            [[next_token_id]],
            dtype=torch.long,
            device=idx.device,
        )
        idx = torch.cat((idx, next_token), dim=1)

    return idx


def generate_with_temperature(model, tokenizer, device, prompt,
                              temperature=1.0, top_k=40, max_new=80):
    """Wrapper ti·ªán d√πng ƒë·ªÉ sinh text t·ª´ 1 prompt."""
    model.eval()
    context_len = GPT_CONFIG_TINY["context_length"]

    # Encode prompt th√†nh token ids
    idx = torch.tensor(
        [tokenizer.encode(prompt)],
        dtype=torch.long,
    ).to(device)

    out_ids = generate(
        model=model,
        idx=idx,
        max_new_tokens=max_new,
        context_length=context_len,
        temperature=temperature,
        top_k=top_k,
    )

    text = tokenizer.decode(out_ids[0].tolist())
    return text


# ================== MAIN ==================

if __name__ == "__main__":
    # 1) Thi·∫øt b·ªã
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)

    # 2) Load d·ªØ li·ªáu domain
    # üí° ƒê·ªîI L·∫†I t√™n file cho ƒë√∫ng n·∫øu b·∫°n kh√¥ng d√πng cooking_corpus.txt
    data_path = "../../data/cooking_corpus.txt"
    full_text = load_domain_text(data_path)

    # T√°ch 90% train, 10% val (gi·ªëng l√∫c train trong my_train_domain.py)
    split_idx = int(0.9 * len(full_text))
    val_text = full_text[split_idx:]

    # 3) Load model ƒë√£ train
    model = build_model(device)

    # 4) T√≠nh loss + perplexity tr√™n validation
    val_loss, val_ppl = compute_perplexity(model, val_text, device)
    print(f"\nValidation loss: {val_loss:.3f}")
    print(f"Validation perplexity: {val_ppl:.2f}")

    # 5) Sinh text v·ªõi nhi·ªÅu prompt v√† temperature kh√°c nhau
    tokenizer = tiktoken.get_encoding("gpt2")

    prompts = [
        "In this recipe, we will",
        "To prepare this dish, first",
        "For a healthy breakfast,",
        "This Vietnamese dish is",
        "For a simple dinner, you can",
    ]

    temperatures = [0.7, 1.0, 1.3]

    for temp in temperatures:
        print("\n" + "=" * 60)
        print(f"TEXT GENERATION WITH TEMPERATURE = {temp}")
        print("=" * 60)
        for p in prompts:
            print(f"\nPrompt: {p}")
            generated_text = generate_with_temperature(
                model=model,
                tokenizer=tokenizer,
                device=device,
                prompt=p,
                temperature=temp,
                top_k=40,          # top-k sampling
                max_new=80,        # s·ªë token sinh th√™m
            )
            print("Output:")
            print(generated_text)
            print("-" * 40)
